{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "import gensim\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import numpy as np\n",
    "#nltk.download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"information_train.csv\", sep = '\\t')\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "data_test = pd.read_csv(\"information_test.csv\", sep = '\\t')\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "sample_sub = pd.read_csv(\"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = data_test.rename(columns= {'set': 'article_set'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "data_test['pub_date'] = pd.to_datetime(data_test['pub_date'])\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.article_set.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_copy = data_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_text(df, text_field):\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\\S+\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\\S+\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \")\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\", \"at\")\n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardize_text(data_test_copy, \"article_title\")\n",
    "standardize_text(data_test_copy, \"abstract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df,text_field):\n",
    "    df['tokens'] = df[text_field]\n",
    "    df['tokens'] = df.tokens.str.split()\n",
    "    stop = stopwords.words('english')\n",
    "    df['tokens'] = df['tokens'].apply(lambda x: [item for item in x if item not in stop])\n",
    "    df['tokens'] = df['tokens'].apply(lambda x: [porter.stem(y) for y in x])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "def get_similarities(df):  # pass text_field_byset\n",
    "    dic = {}\n",
    "    dic_tokens = {}\n",
    "    for x in df.article_set.unique():\n",
    "        final_tokens = [token for token in df.loc[df.article_set == x].tokens] \n",
    "        dictionary = gensim.corpora.Dictionary(final_tokens)\n",
    "        corpus = [dictionary.doc2bow(final_tokens) for final_tokens in final_tokens]\n",
    "        dic_tokens[x] = final_tokens\n",
    "    \n",
    "        tf_idf = gensim.models.TfidfModel(corpus)\n",
    "        sims = gensim.similarities.Similarity('C:/Users/Manohar Battula/Desktop/Innoplexus Hack',tf_idf[corpus],\n",
    "                                      num_features=len(dictionary))\n",
    "        similarities = sims[tf_idf[corpus]]\n",
    "        dic[x]= similarities\n",
    "    return dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top5(sim_dic,art_set):\n",
    "    top_docs = sim_dic[art_set][1].argsort()[::-1][:5]\n",
    "    top_docs_with_score = [(index, round(sim_dic[art_set][1][index], 3))\n",
    "                                for index in top_docs]\n",
    "    print(top_docs_with_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Absrtact Model\n",
    "standardize_text(data_test_copy, 'abstract')\n",
    "abstract_byset = data_test_copy[['abstract','article_set']].copy()\n",
    "preprocessing(abstract_byset,'abstract')\n",
    "abstract_dic= get_similarities(abstract_byset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Title Model \n",
    "standardize_text(data_test_copy, 'article_title')\n",
    "title_byset = data_test_copy[['article_title','article_set']].copy()\n",
    "preprocessing(title_byset,'article_title')\n",
    "title_dic= get_similarities(title_byset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_dfs(df,sim_dic1, sim_dic2): \n",
    "    \n",
    "    ### df --> data_test                                                                                 \n",
    "    #pred =[]\n",
    "    \n",
    "    pmid_top = pd.DataFrame(np.zeros((0,10)), \\\n",
    "                            columns=['top1','top2','top3','top4','top5','top6','top7','top8','top9','top10'])\n",
    "    date_top = pd.DataFrame(np.zeros((0,10)),\\\n",
    "                            columns=['dtop1','dtop2','dtop3','dtop4','dtop5','dtop6','dtop7','dtop8','dtop9','dtop10'])\n",
    "    \n",
    "    for i in df.article_set.unique():\n",
    "        pmid_list_sets = df.loc[df.article_set == i].pmid.tolist()\n",
    "        date_list_sets = df.loc[df.article_set == i].pub_date.tolist()\n",
    "        \n",
    "        combined_sim_list = (np.array(sim_dic1[i]) + np.array(sim_dic2[i])).tolist()\n",
    "        df_pmid = pd.DataFrame(combined_sim_list, columns = pmid_list_sets)\n",
    "        df_date = pd.DataFrame(combined_sim_list, columns = date_list_sets)\n",
    "        x = df_pmid.T\n",
    "        y = df_date.T\n",
    "        \n",
    "        for j in x.columns:\n",
    "            df1row = pd.DataFrame(x.nlargest(10, j).index.tolist(), \\\n",
    "                                  index=['top1','top2','top3','top4','top5','top6','top7','top8','top9','top10']).T\n",
    "            pmid_top = pd.concat([pmid_top.astype(int), df1row], axis=0)\n",
    "            #pmid_top= pmid_top.drop(['top1'], axis=1)\n",
    "            #restolist = pmid_top.values.tolist()\n",
    "        \n",
    "        for j in y.columns:\n",
    "            df1row = pd.DataFrame(y.nlargest(10, j).index.tolist(),\\\n",
    "                                  index=['dtop1','dtop2','dtop3','dtop4','dtop5','dtop6','dtop7','dtop8','dtop9','dtop10']).T\n",
    "            date_top = pd.concat([date_top, df1row], axis=0)\n",
    "            #date_top= date_top.drop(['top1'], axis=1)\n",
    "            #restolist = date_top.values.tolist()\n",
    "        #for k in range(0,len(restolist)):\n",
    "            #pred.append(restolist[k])\n",
    "    return pmid_top,date_top\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## combined model date \n",
    "pmid_top , date_top = get_pred_dfs(data_test, abstract_dic, title_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(pmid_top,date_top):\n",
    "    \n",
    "    pred =[]\n",
    "    for i in range(0,len(pmid_top)):\n",
    "        x=[]\n",
    "        for j in range(0,date_top.shape[1]):\n",
    "            if ((date_top.iloc[i][0] > date_top.iloc[i][j]) and len(x)<3):\n",
    "                y = pmid_top.iloc[i][j] \n",
    "                x.append(y) \n",
    "        pred.append(x)\n",
    "    preddf = pd.DataFrame()\n",
    "    preddf['pmid'] = data_test.pmid\n",
    "    preddf['ref_list'] = pred\n",
    "    prediction = pd.merge(test,preddf, on='pmid', how='left')\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = get_prediction(pmid_top,date_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def get_similarities_tfidf(df,text_field):     # pass data_test\n",
    "    dic = {}\n",
    "    #dic_tokens = {}\n",
    "    for x in df.article_set.unique():\n",
    "        vectorizer = TfidfVectorizer(max_df=0.3, lowercase = True, ngram_range= (1,3),\n",
    "                             min_df=1, stop_words='english')\n",
    "                             \n",
    "        \n",
    "        corpus = [y for y in df.loc[df.article_set == x][text_field]]\n",
    "        dtm = vectorizer.fit_transform(corpus)\n",
    "        tfidf_sims = gensim.similarities.Similarity('C:/Users/Manohar Battula/Desktop/Innoplexus Hack',\n",
    "                                                    dtm,num_features=dtm.shape[1])\n",
    "        similarities = tfidf_sims[dtm]\n",
    "        dic[x]= similarities\n",
    "    return dic\n",
    "\n",
    "title_dic_tfidf = get_similarities_tfidf(data_test,'article_title')\n",
    "abstract_dic_tfidf = get_similarities_tfidf(data_test,'abstract')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "### Lsi Model\n",
    "\n",
    "from gensim import corpora, models, similarities\n",
    "def get_similarities_lsi(df):  # pass article_title_byset\n",
    "    dic = {}\n",
    "    dic_tokens = {}\n",
    "    for x in df.article_set.unique():\n",
    "        final_tokens = [token for token in df.loc[df.article_set == x].tokens] \n",
    "        dictionary = gensim.corpora.Dictionary(final_tokens)\n",
    "        corpus = [dictionary.doc2bow(final_tokens) for final_tokens in final_tokens]\n",
    "        dic_tokens[x] = final_tokens\n",
    "    \n",
    "        tf_idf = gensim.models.TfidfModel(corpus)\n",
    "        #corpus_tfidf = tf_idf[corpus]\n",
    "        lsi = models.LsiModel(corpus)\n",
    "        sims = gensim.similarities.MatrixSimilarity(lsi[corpus])\n",
    "                                                   \n",
    "                                      \n",
    "        similarities = sims[lsi[corpus]]\n",
    "        dic[x]= similarities\n",
    "    return dic\n",
    "    \n",
    "title_dic_lsi = get_similarities_lsi(article_title_byset)\n",
    "abstract_dic_lsi = get_similarities_lsi(abstract_byset)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "### word2vec\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "preprocessing(article_title_byset,'article_title')\n",
    "sentences = [token for token in title_byset.tokens]\n",
    "\n",
    "word2vec_model = Word2Vec(sentences, min_count = 1)\n",
    "\n",
    "def document_vector(word2vec_model, doc):\n",
    "    # remove out-of-vocabulary words\n",
    "    doc = [word for word in doc if word in word2vec_model.wv.vocab]\n",
    "    return np.mean(word2vec_model[doc], axis=0)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "sims = cosine_similarity(np.array([document_vector(word2vec_model, doc)\n",
    "                                                       for doc in sentences]))\n",
    "\n",
    "def get_similarities_word2vec(df):  # EX. pass title_byset, abstract_byset\n",
    "    dic = {}\n",
    "    dic_tokens = {}\n",
    "    sentences = [token for token in df.tokens]\n",
    "    for x in df.article_set.unique():\n",
    "        corpus = [token for token in df.loc[df.article_set == x].tokens] \n",
    "        word2vec_model = Word2Vec(sentences, min_count = 1)\n",
    "        sims = cosine_similarity(np.array([document_vector(word2vec_model, doc)\n",
    "                                                       for doc in corpus]))\n",
    "        dic[x]= sims\n",
    "    return dic\n",
    "\n",
    "title_dic_w2v = get_similarities_word2vec(title_byset)\n",
    "abstract_dic_w2v = get_similarities_word2vec(abstract_byset)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
